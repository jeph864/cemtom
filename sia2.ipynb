{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define the path to the directory you want to switch to\n",
    "new_directory = \"baselines/Cluster-Analysis\"\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a657ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of the notebook: 61.01 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Get the current process\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Memory usage in MB\n",
    "mem = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "print(f\"Memory usage of the notebook: {mem:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1bd63a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fasttext.util\n",
    "import fasttext\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BaseWordEmbedder:\n",
    "    def __init__(self, embedding_model=None):\n",
    "        self.embedding_model = embedding_model\n",
    "    def embed(self, words):\n",
    "        raise NotImplemented(\"Subclass should implement the method\")\n",
    "class FasttextEmbedder(BaseWordEmbedder):\n",
    "    def __init__(self, embedding_model=None, path = None):\n",
    "        super().__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.model_path = path\n",
    "        self.model = None\n",
    "        if self.model_path is not None and os.path.exists(self.model_path):\n",
    "            self.model = fasttext.load_model(self.model_path)\n",
    "        elif self.embedding_model is not None:\n",
    "            self.model = self.embedding_model\n",
    "        else:\n",
    "            raise ValueError(\"No Model or path given\")\n",
    "    def embed(self, words):\n",
    "        word_embeddings = []\n",
    "        for word in words:\n",
    "            word_embeddings.append(self.model.get_word_vector(word))\n",
    "        return np.array(word_embeddings)\n",
    "            \n",
    "def get_word_embedding_model(name = None, model= None, path = None):\n",
    "    if name == \"fasttext\":\n",
    "        return FasttextEmbedder(embedding_model= model, path=path)\n",
    "    else:\n",
    "        return BaseWordEmbedder()\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "class DimReductionBase:\n",
    "    def __init__(self, model = None, nr_dims = None):\n",
    "        self.model = model\n",
    "        self.nr_dims = nr_dims\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "class PCAReduction(DimReductionBase):\n",
    "    def __init__(self, nr_dims = 5):\n",
    "        super().__init__()\n",
    "        self.nr_dims = nr_dims\n",
    "        self.model = PCA(n_components = self.nr_dims)\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        pass\n",
    "    def fit_transform(self, X):\n",
    "        return self.model.fit_transform(X)\n",
    "    \n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "class ClusterBase:\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "class KMeansCluster(ClusterBase):\n",
    "    def __init__(self, n_clusters = None, random_state= 42):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.model = KMeans(n_clusters=self.n_clusters, random_state= self.random_state)\n",
    "        self.m_clusters = None\n",
    "    def fit(self, data, weights = None):\n",
    "        return self.model.fit(data, sample_weight = weights)\n",
    "    def transform(self, data, weights = None):\n",
    "        return self.model.predict(data, sample_weight=weights)\n",
    "    def fit_transform(self, data, weights=None):\n",
    "        self.model = self.fit(data, weights)\n",
    "        self.m_clusters = self.transform(data, weights)\n",
    "        centers = np.array(self.model.cluster_centers_)\n",
    "        return self.m_clusters, centers\n",
    "    \n",
    "import pandas as pd    \n",
    "class Sia:\n",
    "    def __init__(self,\n",
    "                 vocab = None,\n",
    "                 embedding_model_name = \"fasttext\",\n",
    "                 embedding_model = None, \n",
    "                 vectorizer = None,\n",
    "                 nr_dimensions = None,\n",
    "                 reduction_model = None,\n",
    "                 nr_topics = 10,\n",
    "                 rerank = None,\n",
    "                 weighting = None\n",
    "                ):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model = embedding_model\n",
    "        self.nr_dimensions = nr_dimensions\n",
    "        self.nr_topics = nr_topics\n",
    "        self.vocab = vocab\n",
    "        self.vocab_embeddings = None\n",
    "        self.vectorizer = vectorizer\n",
    "        if self.vectorizer is None:\n",
    "            self.vectorizer = CountVectorizer()\n",
    "            pass\n",
    "        self.reduction_model = reduction_model\n",
    "        if self.reduction_model is None:\n",
    "            self.reduction_model = PCAReduction(nr_dimensions)\n",
    "        if self.vocab is None:\n",
    "            pass#raise TypeError(\"Vocab should not be NoneType\")\n",
    "        self.cluster_model = None\n",
    "        self._labels = None\n",
    "        self.rerank = rerank\n",
    "        self.weighting = weighting\n",
    "        self.feat_mat = None\n",
    "            \n",
    "    def fit(self, documents, vocab = None, embeddings =None,y = None):\n",
    "        self.fit_transform(documents = documents, vocab=vocab, embeddings=embeddings, y = y)\n",
    "        return self\n",
    "    def fit_transform(self, documents, vocab=None, embeddings=None, y = None):\n",
    "        if vocab is not None:\n",
    "            self.vocab = vocab\n",
    "        elif self.vocab is None:\n",
    "            if self.vectorizer is None:\n",
    "                raise ValueError(\"Please provide the vectorozer\")\n",
    "            else:\n",
    "                #self.vocab = self.vectorizer.get_feature_names_out()\n",
    "                #if len(self.vocab) > 0:\n",
    "                #    self.vocab = self.vectorizer.get_feature_names_out()\n",
    "                #else:\n",
    "                self.feat_mat = self.vectorizer.fit_transform(documents)\n",
    "                self.vocab = self.vectorizer.get_feature_names_out()\n",
    "                \n",
    "        if documents is not None:\n",
    "            #check_documents_type(documents)\n",
    "            #check_embeddings_shape(embeddings, self.vocab)\n",
    "            pass\n",
    "        if self.vocab is None:\n",
    "            pass\n",
    "            \n",
    "        docs_ids = range(len(documents))\n",
    "        docs = pd.DataFrame({\"Document\": documents,\n",
    "                          \"ID\": docs_ids,\n",
    "                          \"Topic\": None})\n",
    "        vocab_embeddings = None\n",
    "        if embeddings is None and self.embedding_model is None:\n",
    "            self.embedding_model = get_word_embedding_model(name = \"fasttext\", path=\"embeds/fasttext/wiki.en.bin\")\n",
    "            \n",
    "        elif self.embedding_model is not None:\n",
    "            print(\"creating vocabulary embeddings\")\n",
    "            vocab_embeddings = self.embedding_model.embed(self.vocab)\n",
    "        else:\n",
    "            vocab_embeddings = embeddings\n",
    "        self.vocab_embeddings = vocab_embeddings \n",
    "        print(f\"vocab embeddings shape: {self.vocab_embeddings.shape}; documents shape : {len(documents)}\")\n",
    "        if self.reduction_model is not None:\n",
    "            print(\"reducing the dimensions\")\n",
    "            vocab_embeddings = self.reduction_model.fit_transform(vocab_embeddings)\n",
    "        elif self.nr_dimensions is not None:\n",
    "            print(\"reducing the dimensions with PCA\")\n",
    "            self.reduction_model = PCAReduction(nr_dims = self.nr_dimensions)\n",
    "            vocab_embeddings = self.reduction_model.fit_transform(vocab_embeddings)\n",
    "        #weighting\n",
    "        weights = None\n",
    "        if self.weighting is not None:\n",
    "            if self.weighting == \"wgt\":\n",
    "                weights = self.feat_mat.toarray().sum(axis=0)\n",
    "                print(f\"weights shape(before) : {weights[0].shape}\")\n",
    "                print(np.squeeze(weights))\n",
    "                #scale\n",
    "        if weights is not None:\n",
    "            scaled_weights = 1/(1+np.exp(weights))\n",
    "            weights = scaled_weights.reshape(-1)\n",
    "            print(f\"weights shape : {weights.shape}\")\n",
    "            \n",
    "            \n",
    "        #start clustering\n",
    "        self.cluster_model = KMeansCluster(n_clusters = self.nr_topics)\n",
    "        clusters, centers = self.cluster_model.fit_transform(vocab_embeddings, weights=weights)\n",
    "        print(f\"Finished clustering: {clusters.shape}, centers: {centers.shape}\")\n",
    "    \n",
    "        sorted_tops = self._sort_closest_centers(centers, clusters, vocab_embeddings)\n",
    "        print(f\"Shape of the sorted topk words {sorted_tops.shape}\")\n",
    "        top_k_indices = None\n",
    "        if self.rerank:\n",
    "            top_k_indices = self._find_top_k_words(100, sorted_tops)\n",
    "        else:\n",
    "            top_k_indices = self._find_top_k_words(10, sorted_tops)\n",
    "        ##rerank\n",
    "        self.labels_ = clusters\n",
    "        self.top_k = top_k_indices\n",
    "        if self.rerank is not None:\n",
    "            print(\"reranking\")\n",
    "            self.top_k = self._rerank(np.array(top_k_indices), documents)\n",
    "        return self.top_k\n",
    "    def get_topic_words(self):\n",
    "        if self.top_k is None:\n",
    "            raise ValueError(\"Fit the model first\")\n",
    "        words = []\n",
    "        for topic in self.top_k:\n",
    "            words.append(self.vocab[topic])\n",
    "        return words\n",
    "        \n",
    "    def _rerank(self, topic_word_indices, documents, feat_mat = None, k= 20):\n",
    "        if feat_mat is None:\n",
    "            feat_mat = self.vectorizer.transform(documents).toarray()\n",
    "        feat_mat = feat_mat.T\n",
    "        topk = []\n",
    "        print(f\"feat matrix: {feat_mat.shape}/ topic word indices: {topic_word_indices.shape}\")\n",
    "        for topic_words_idx in topic_word_indices:\n",
    "            count = feat_mat[topic_words_idx].sum(axis=1)\n",
    "            count = count.argsort()[-k:][::-1].astype(int)\n",
    "            topk.append(topic_words_idx[count])\n",
    "        #print(topk)\n",
    "        return topk\n",
    "    #def \n",
    "        \n",
    "    def _get_document_stats(self, weighting = None):\n",
    "        pass\n",
    "    def _find_top_k_words(self, k, top_vals):\n",
    "        topk_words = []\n",
    "        for top in range(top_vals.shape[0]):\n",
    "            ind, unique = [], set()\n",
    "            for i in top_vals[top]:\n",
    "                word = self.vocab[i]\n",
    "                #print(word)\n",
    "                if word not in unique:\n",
    "                    ind.append(i)\n",
    "                    unique.add(word)\n",
    "                    if len(unique) == k:\n",
    "                        break\n",
    "            topk_words.append(ind)\n",
    "        return topk_words\n",
    "    def _sort_closest_centers(self, centers, clusters, vocab_embedding, k=20):\n",
    "        top_idx = []\n",
    "        for topic in range(centers.shape[0]):\n",
    "            diic = np.where(clusters==topic)[0]\n",
    "            dist = np.sum((vocab_embedding[diic]-centers[topic])**2, axis=1)\n",
    "            topk = dist.argsort()[:k]\n",
    "            #print(words[diic[topk]])\n",
    "            #print(diic[topk].shape)\n",
    "            top_idx = np.vstack((top_idx, diic[topk])) if topic > 0 else diic[topk]\n",
    "        return top_idx\n",
    "def sia_dataset_preprocess(docs):\n",
    "    vocab = set()\n",
    "    mapping = {}\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                vocab.add(word)\n",
    "                mapping[word] = set()\n",
    "                mapping[word].add(i)\n",
    "            else:\n",
    "                mapping[word].add(i)\n",
    "    return mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "156931c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dev = sia_dataset_preprocess(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45df23b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to extract the dataset\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:00<00:00, 228431.80it/s]\n",
      "100%|██████████| 18846/18846 [00:04<00:00, 4690.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 18846\n",
      "vocab created 20225\n"
     ]
    }
   ],
   "source": [
    "from cemtom.dataset._20newgroup import fetch_dataset\n",
    "from cemtom.preprocessing import Preprocessor\n",
    "dataset = fetch_dataset(remove=(\"headers\", \"quotes\", \"footers\"))\n",
    "token_dict={\n",
    "    \"doc_start_token\": '<s>',\n",
    "    \"doc_end_token\":'</s>',\n",
    "    \"unk_token\":'<unk>',\n",
    "    \"email_token\":'<email>',\n",
    "    \"url_token\":'<url>',\n",
    "    \"number_token\":'<number>',\n",
    "    \"alpha_num_token\":'<alpha_num>'\n",
    "}\n",
    "preprocessor = Preprocessor(stopwords_list=\"english\", remove_spacy_stopwords = False,\n",
    "                            token_dict=token_dict, use_spacy_tokenizer=True,min_df=5,\n",
    "                           max_df = 0.80)\n",
    "#preprocessor = Preprocessor(stopwords_list=\"english\", remove_spacy_stopwords = False)\n",
    "ds = preprocessor.preprocess(None,dataset=dataset)\n",
    "\n",
    "train_corpus, test_corpus = ds.get_partitioned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf345e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = FasttextEmbedder(embedding_model=ft)\n",
    "#embedding_model = get_word_embedding_model(name = \"fasttext\", path=\"embeds/fasttext/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220904d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc for doc in train_corpus+test_corpus if \"aaaaarrrrgh\" in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7e4dfc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating vocabulary embeddings\n",
      "vocab embeddings shape: (18381, 300); documents shape : 15507\n",
      "reducing the dimensions\n",
      "weights shape(before) : ()\n",
      "[33 36 70 ...  9  5 11]\n",
      "weights shape : (18381,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/9294634.0.tmpdir/ipykernel_324525/993910926.py:172: RuntimeWarning: overflow encountered in exp\n",
      "  scaled_weights = 1/(1+np.exp(weights))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished clustering: (18381,), centers: (20, 10)\n",
      "Shape of the sorted topk words (20, 20)\n",
      "reranking\n",
      "feat matrix: (18381, 15507)/ topic word indices: (20, 20)\n"
     ]
    }
   ],
   "source": [
    "model_output = {}\n",
    "vocab = None#preprocessor.vectorizer.get_feature_names_out()\n",
    "sia_model = Sia(vocab, embedding_model = embedding_model, vectorizer=preprocessor.vectorizer,nr_dimensions=10, \n",
    "                nr_topics = 20, weighting=\"wgt\", rerank = \"tf\"\n",
    "               )\n",
    "model_output['model'] = sia_model.fit_transform(train_corpus)\n",
    "model_output['topics'] = sia_model.get_topic_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47323061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.evaluation_metrics.coherence_metrics import Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab12f48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.39066353673805476"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs_split = [doc.split() for doc in test_corpus[:]]\n",
    "coherence = Coherence(texts = test_docs_split)\n",
    "c_npmi = coherence.score(model_output)\n",
    "c_npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4e66804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2737\n",
      "-0.88134 iran bosnian karabagh iranian serbian serbia cypriot rus kars poland bulgaria golan nicosia afghanistan sarajevo komsomol shah hungary salah bulgarian\n",
      "-0.93956 fan pop movie movies comic episode hype hop episodes pals teapot videos cartoon mags slim rap dorm hardcore rave banger\n",
      "-0.94623 davidians propaganda believers fascist commandment fundamentalist fundamentalists inquisition aryan communism caste baptize crusades crusade religiously westerners anarchist testimonies sectarian imperialism\n",
      "-0.66024 period land middle cross beginning hundred remains numerous twenty collected twelve thirty fifty partly risen sixty eighty collectively ninety onwards\n",
      "-0.87268 world la plays film paris prize swedish les breton italian del rawlinson dutch ulysses mater kaiser everest figaro contra germano\n",
      "-0.87213 truth reality desire thoughts minds minded genuine transgression contemplate undeniable satire eloquent dilemma sincerity recollection anachronism anecdote overtly actuality earnestly\n",
      "-0.84991 fire fighting forced battle possession strikes retreat unarmed halt guards ensuing battles guarded eyewitnesses sinking sever storming demolition decimated bombard\n",
      "-0.98179 api playback xloadimage assembler calculator xremote imagewriter toplevel runtime ssd internals doublespace hdtv winsock convertor pspice rng porting keymap defragmented\n",
      "-0.79722 season players rangers flyers wins hawks giants tigers seasons bench starter mariners roster soccer doubles trophy skating panthers rookies cowboys\n",
      "-1.0 blame believing rumors suspicion troubles unhappy notorious rumour pleading deceived willingly displeasure futile rumours uneasy anxious dire adamant hesitation pretended\n",
      "-0.79311 smith patrick anderson lopez roberts stevens larkin turner sanders barnes snyder bates fletcher hayward lind leblanc hanson gorman whitmore cameron\n",
      "-0.86478 dc md ac rt sb gc dm ea cbr av pw af dw wais ej uni mst tac tg pk\n",
      "-0.80611 individual highly instance possibility differences manner demonstrate possibilities importantly identifying satisfactory demonstrating stressed continual lapse dictates twofold conversely satisfactorily incidental\n",
      "-0.87974 pin wire clock floor wheel steel deck pipe pipes steam drill rolls stripe bungee quieter binoculars rails balloons lifts paddle\n",
      "-0.94158 hot dogs dirt grass wash fresh pot bottle nut brush turtles bake purple freshly fur tatoos monkeys paws squat skins\n",
      "-0.89112 reason answer absolutely presume needless admittedly thier rationale realy unsupported truely unreliable reread apparantly wether writting becasue somone factoid otoh\n",
      "-0.92422 frequency flow static frequencies sequences cycles lens vacuum smoothing combinations envelope scaling interval flux saturation externally bottleneck normals amplified polarized\n",
      "-0.95072 disease breast infants infectious dose headaches sperm acute viral fetus migraines haldol semen sufferers genetically injections strains marrow treatable morphine\n",
      "-0.77691 face turns suddenly quiet sight crack calm breaks barely sounded touched shake bored instantly stare unlucky purposely stray stumble stumbling\n",
      "-0.83004 legal act legally approval ownership participants participation voluntary mandatory participating promotion admission adoption receipt reservations exams organizers exchanges trusts policing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.87297"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_corpus))\n",
    "sia_npmi = average_npmi_topics(model_output['topics'], 20, sia_dataset_preprocess(test_corpus[:]), len(test_corpus[:]))\n",
    "sia_npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34181d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47b582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b01c149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = get_word_embedding_model(name = \"fasttext\", path=\"embeds/fasttext/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c68ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.FasttextEmbedder at 0x151fbf6c1820>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7ce93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"prayer\", \"model\", \"computer\", \"science\",\"bible\", \"something\", \"ability\", \"hence\",  \"jesus\", \"god\"]\n",
    "dim_reduction = PCAReduction(nr_dims = 2)\n",
    "words_red = dim_reduction.fit_transform(model.embed(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc921b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 0, 1, 0, 0, 0, 1, 1], dtype=int32),\n",
       " array([[-1.3804448 , -0.05626277],\n",
       "        [ 2.0706675 ,  0.0843941 ]], dtype=float32))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeansCluster(n_clusters = 2)\n",
    "m,c = km.fit_transform(words_red)\n",
    "m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3e75f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9, 8, 0, 4]), array([3, 2, 0, 1]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = 1\n",
    "diic = np.where(m==topic)[0]\n",
    "dist = np.sum((words_red[diic]-c[topic])**2, axis=1)\n",
    "topk = dist.argsort()\n",
    "diic[topk], topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39a1872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def number_tokenizer(document):\n",
    "    # Regular expression pattern for numbers\n",
    "    pattern = r'\\b\\d+(?:,\\d{3})*(?:\\.\\d+)?\\b'\n",
    "    \n",
    "    # Replace numbers with <number>\n",
    "    doc_no_numbers = re.sub(pattern, '<number>', document)\n",
    "    \n",
    "    # Tokenize by splitting on whitespace\n",
    "    tokens = doc_no_numbers.split()\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f31492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f699211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248498b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b783ddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((21624,), (20, 5))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20, 15),\n",
       " array(['labor', 'labour', 'general', 'kingdom', 'defence', 'recognized',\n",
       "        'represented', 'states', 'members', 'formally', 'nurses',\n",
       "        'students', 'defense', 'representing', 'supervision'], dtype=object))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "documents = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "\n",
    "# Example documents\n",
    "documents = documents\n",
    "nr_topics = 20\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=5, stop_words=\"english\", strip_accents=\"ascii\", lowercase=True, token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "words=vectorizer.get_feature_names_out()\n",
    "feat_mat = X.toarray().T\n",
    "dim_reduction = PCAReduction(nr_dims = 5)\n",
    "words_red = dim_reduction.fit_transform(embedding_model.embed(words))\n",
    "km = KMeansCluster(n_clusters = nr_topics)\n",
    "m,c = km.fit_transform(words_red)\n",
    "print((m.shape,c.shape))\n",
    "top_idx = []\n",
    "top_words = []\n",
    "sia_model._sort_closest_centers(c, m, words_red)\n",
    "for topic in range(nr_topics): \n",
    "    diic = np.where(m==topic)[0]\n",
    "    dist = np.sum((words_red[diic]-c[topic])**2, axis=1)\n",
    "    topk = dist.argsort()[:15]\n",
    "    #print(words[diic[topk]])\n",
    "    #print(diic[topk].shape)\n",
    "    top_idx = np.vstack((top_idx, diic[topk])) if topic > 0 else diic[topk]\n",
    "    top_words.append(words[diic[topk]])\n",
    "#top_idx = np.array(top_idx)\n",
    "top_idx.shape, top_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59a1364f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = feat_mat.sum(axis=0)\n",
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66d3280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(topic_word_indices, feat_mat):\n",
    "    topk = []\n",
    "    print(f\"feat matrix: {feat_mat.shape}/ topic word indices: {topic_word_indices.shape}\")\n",
    "    for topic_words_idx in topic_word_indices:\n",
    "        count = feat_mat[topic_words_idx].sum(axis=1)\n",
    "        count = count.argsort()[-10:][::-1].astype(int)\n",
    "        topk.append(topic_words_idx[count])\n",
    "    return topk\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c34c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat matrix: (21624, 18846)/ topic word indices: (20, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 7082,  5043,  1575, 19361,  2941,  1257,  8583, 16283, 14203,\n",
       "         1752]),\n",
       " array([ 7118, 13790, 19196, 17219, 15322,  8983,  5690,  3140, 17483,\n",
       "         1461]),\n",
       " array([ 1442, 11908, 11559, 13125, 19691, 15011,   135,  3851, 18878,\n",
       "         6728]),\n",
       " array([12037, 16409,  2014, 15228,  6736,  5962, 16947,  6922,  4312,\n",
       "        10573]),\n",
       " array([12583, 19459,  3272, 12867, 16295, 16886, 21040, 14729, 11506,\n",
       "        21039]),\n",
       " array([ 2555, 15370,  5642, 20662, 13924, 21304,  6459, 16565, 21591,\n",
       "        19949]),\n",
       " array([18596, 14962,   559, 19319, 16592, 10216,   428,  1319,  3018,\n",
       "        19209]),\n",
       " array([  807,  7067,  4990, 17035,  5906, 15497, 18368,  6434,  5161,\n",
       "        10018]),\n",
       " array([15085, 13764, 10106, 13472,  9792,   594,  9216, 19253, 18282,\n",
       "        11213]),\n",
       " array([ 9452,  6418, 13548, 20870,  5452, 20150, 13906, 15535,  8280,\n",
       "         6369]),\n",
       " array([   14, 16644, 14293,   373, 10804,  6936, 12646, 10030,  6989,\n",
       "        13295]),\n",
       " array([ 6298,  8214,  4470, 15410, 13066,  4552, 14518, 15160,  3745,\n",
       "        14175]),\n",
       " array([12331,  8247, 20597, 13974,  4027,  3844, 10572,  3475,  5599,\n",
       "        14658]),\n",
       " array([ 1806,    72,  9115,  7663, 13678,  6915,  5505, 14676,  2096,\n",
       "          383]),\n",
       " array([21406, 12389, 13364, 12457,   226, 19096,  9307, 11745, 10060,\n",
       "        21348]),\n",
       " array([ 5472,  4185, 18693,   918, 16385,   677, 18612,  1171, 14693,\n",
       "         5548]),\n",
       " array([14922, 18224,  9441,  1899, 17917,  5297,  2963, 12663,  9073,\n",
       "         3078]),\n",
       " array([21266, 16601,  6336, 16876,  3920,  9573, 19706, 17129,  7423,\n",
       "        12127]),\n",
       " array([12872, 12359,  2517,  6646, 10595, 13890, 20827, 10439, 12863,\n",
       "         3007]),\n",
       " array([17187, 19814, 12792,  4304, 13428,  1891,  1504,  7409,  3604,\n",
       "         6078])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tki = sia_model._sort_closest_centers(c, m, words_red)\n",
    "#f_mat = vectorizer.transform(documents[:1000])\n",
    "#print(f_mat.T.shape)\n",
    "reranked = rerank(tki, feat_mat)\n",
    "#reranked\n",
    "#f_mat.T.sum(axis = 1).reshape(-1).shape\n",
    "reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f9e077c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3565, 100), (10, 15))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_mat.shape, top_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fe78c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 5, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2]),\n",
       " array([ 2, 14,  9,  5,  3, 13, 12, 11, 10,  8]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = feat_mat[top_idx[0]].sum(axis=1)\n",
    "count, count.argsort()[-10:][::-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c1ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56411f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e2c2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch-old/abijuru/thesis/tms/atlas/baselines/Cluster-Analysis\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "46648f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "#from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pdb\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#from spherecluster import SphericalKMeans\n",
    "#from spherecluster import VonMisesFisherMixture\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.stats\n",
    "\n",
    "def PCA_dim_reduction(intersection, dim):\n",
    "    intersection = intersection - np.mean(intersection, axis = 0)\n",
    "    sigma = np.cov(intersection.T)\n",
    "    eigVals, eigVec = np.linalg.eig(sigma)\n",
    "    sorted_index = eigVals.argsort()[::-1]\n",
    "    eigVals = eigVals[sorted_index]\n",
    "    eigVec = eigVec[:,sorted_index]\n",
    "    eigVec = eigVec[:,:dim]\n",
    "    transformed = intersection.dot(eigVec)\n",
    "    return transformed\n",
    "\n",
    "def TSNE_dim_reduction(intersection, dim):\n",
    "    X_embedded = TSNE(n_components=dim).fit_transform(intersection)\n",
    "    return X_embedded\n",
    "\n",
    "def Agglo_model(vocab_embeddings, topics, rand):\n",
    "    agglo = AgglomerativeClustering(n_clusters=topics).fit(vocab_embeddings)\n",
    "    m_clusters = agglo.labels_\n",
    "    return m_clusters, find_words_for_cluster(m_clusters, topics)\n",
    "\n",
    "def DBSCAN_model(vocab_embeddings, e=0.5):\n",
    "    dbscan = DBSCAN(eps=e, min_samples=10).fit(vocab_embeddings)\n",
    "    m_clusters = dbscan.labels_\n",
    "    clusters = len(np.unique(m_clusters[m_clusters>= 0]))\n",
    "    return m_clusters, find_words_for_cluster(m_clusters, clusters)\n",
    "\n",
    "def SpectralClustering_Model(vocab_embeddings, topics, rand, pmi):\n",
    "    precomp = rbf_kernel(vocab_embeddings)\n",
    "\n",
    "    #print(precomp)\n",
    "    #pmax, pmin = pmi.max(), pmi.min()\n",
    "    #pmi = (pmi - pmin)/(pmax - pmin)\n",
    "\n",
    "    #precomp = precomp * pmi\n",
    "    #print(precomp)\n",
    "\n",
    "    #nearest_neighbors\n",
    "    #precomputed\n",
    "    SC = SpectralClustering(n_clusters=topics, random_state=rand, affinity = \"nearest_neighbors\").fit(vocab_embeddings)\n",
    "    m_clusters = SC.labels_\n",
    "\n",
    "    return m_clusters, find_words_for_cluster(m_clusters, topics)\n",
    "\n",
    "def KMedoids_model(vocab_embeddings, vocab, topics,  rand):\n",
    "    kmedoids = KMedoids(n_clusters=topics, random_state=rand).fit(vocab_embeddings)\n",
    "    m_clusters = kmedoids.predict(vocab_embeddings)\n",
    "    centers = np.array(kmedoids.cluster_centers_)\n",
    "    indices = []\n",
    "\n",
    "    for i in range(20):\n",
    "        topk_vals = sort_closest_center(centers[i], m_clusters, vocab_embeddings, i)\n",
    "        indices.append(find_top_k_words(100, topk_vals, vocab))\n",
    "\n",
    "    return m_clusters, indices\n",
    "\n",
    "def KMeans_model(vocab_embeddings, vocab, topics, rerank, rand, weights):\n",
    "    kmeans = KMeans(n_clusters=topics, random_state=rand).fit(vocab_embeddings, sample_weight=weights)\n",
    "    m_clusters = kmeans.predict(vocab_embeddings, sample_weight=weights)\n",
    "    centers = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    for i in range(topics):\n",
    "        topk_vals = sort_closest_center(centers[i], m_clusters, vocab_embeddings, i)\n",
    "        if rerank:\n",
    "            indices.append(find_top_k_words(100, topk_vals, vocab))\n",
    "        else:\n",
    "            indices.append(find_top_k_words(10, topk_vals, vocab))\n",
    "        #print(indices)\n",
    "    return m_clusters, indices\n",
    "\n",
    "\n",
    "def SphericalKMeans_model(vocab_embeddings,vocab,topics, rerank, rand, weights):\n",
    "\n",
    "    spkmeans = SphericalKMeans(n_clusters=topics, random_state=rand).fit(vocab_embeddings, sample_weight=weights)\n",
    "    m_clusters = spkmeans.predict(vocab_embeddings,  sample_weight=weights)\n",
    "    centers = np.array(spkmeans.cluster_centers_)\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    for i in range(topics):\n",
    "        topk_vals = sort_closest_cossine_center(centers[i], m_clusters, vocab_embeddings, i)\n",
    "        if rerank:\n",
    "            indices.append(find_top_k_words(100, topk_vals, vocab))\n",
    "        else:\n",
    "            indices.append(find_top_k_words(10, topk_vals, vocab))\n",
    "        #print(indices)\n",
    "    return m_clusters, indices\n",
    "\n",
    "def GMM_model(vocab_embeddings, vocab,  topics, rerank, rand):\n",
    "    GMM = GaussianMixture(n_components=topics, random_state=rand).fit(vocab_embeddings)\n",
    "    indices = []\n",
    "    for i in range(GMM.n_components):\n",
    "        density = scipy.stats.multivariate_normal(cov=GMM.covariances_[i], mean=GMM.means_[i]).logpdf(vocab_embeddings)\n",
    "        topk_vals = density.argsort()[-1*len(density):][::-1].astype(int)\n",
    "        if rerank:\n",
    "            indices.append(find_top_k_words(100, topk_vals, vocab))\n",
    "        else:\n",
    "            indices.append(find_top_k_words(10, topk_vals, vocab))\n",
    "\n",
    "    return GMM.predict(vocab_embeddings), indices\n",
    "\n",
    "def VonMisesFisherMixture_Model(vocab_embeddings, vocab, topics, rerank, rand):\n",
    "    #vmf_soft = VonMisesFisherMixture(n_clusters=topics, posterior_type='hard', n_jobs=-1, random_state=rand).fit(vocab_embeddings)\n",
    "    print(\"fitting vmf...\")\n",
    "    vmf_soft = VonMisesFisherMixture(n_clusters=topics, posterior_type='soft', n_jobs=-1, random_state=rand).fit(vocab_embeddings)\n",
    "\n",
    "    llh = vmf_soft.log_likelihood(vocab_embeddings)\n",
    "    indices = []\n",
    "    for i in range(topics):\n",
    "\n",
    "        topk_vals = llh[i, :].argsort()[::-1].astype(int)\n",
    "        if rerank:\n",
    "            indices.append(find_top_k_words(100, topk_vals, vocab))\n",
    "        else:\n",
    "            indices.append(find_top_k_words(10, topk_vals, vocab))\n",
    "\n",
    "    return vmf_soft.predict(vocab_embeddings), indices\n",
    "\n",
    "def sort_closest_center(center_vec, m_clusters,vocab_embeddings, c_ind):\n",
    "    data_idx_within_i_cluster = np.array([ idx for idx, clu_num in enumerate(m_clusters) if clu_num == c_ind ])\n",
    "    one_cluster_tf_matrix = np.zeros((len(data_idx_within_i_cluster) , center_vec.shape[0]))\n",
    "\n",
    "    for row_num, data_idx in enumerate(data_idx_within_i_cluster):\n",
    "        one_row = vocab_embeddings[data_idx]\n",
    "        one_cluster_tf_matrix[row_num] = one_row\n",
    "\n",
    "    dist_X =  np.sum((one_cluster_tf_matrix - center_vec)**2, axis = 1)\n",
    "    #topk = min(10, len(data_idx_within_i_cluster))\n",
    "    #topk_vals = dist_X.argsort()[:topk].astype(int)\n",
    "\n",
    "    topk_vals = dist_X.argsort().astype(int)\n",
    "    topk_vals = data_idx_within_i_cluster[topk_vals]\n",
    "\n",
    "    return topk_vals\n",
    "\n",
    "def sort_closest_cossine_center(center_vec, m_clusters,vocab_embeddings, c_ind):\n",
    "        data_idx_within_i_cluster = np.array([ idx for idx, clu_num in enumerate(m_clusters) if clu_num == c_ind ])\n",
    "        one_cluster_tf_matrix = np.zeros((len(data_idx_within_i_cluster) , center_vec.shape[0]))\n",
    "\n",
    "        for row_num, data_idx in enumerate(data_idx_within_i_cluster):\n",
    "            one_row = vocab_embeddings[data_idx]\n",
    "            one_cluster_tf_matrix[row_num] = one_row\n",
    "\n",
    "        dist_X =  (cosine_similarity(one_cluster_tf_matrix, center_vec.reshape(1, -1))).squeeze()\n",
    "        dist_X = 2.0*(1.0-dist_X)\n",
    "        #topk = min(10, len(data_idx_within_i_cluster))\n",
    "        #topk_vals = dist_X.argsort()[:topk].astype(int)\n",
    "\n",
    "        topk_vals = dist_X.argsort().astype(int)\n",
    "        topk_vals = data_idx_within_i_cluster[topk_vals]\n",
    "\n",
    "        return topk_vals\n",
    "\n",
    "def find_top_k_words(k, top_vals, vocab):\n",
    "    ind = []\n",
    "    unique = set()\n",
    "    for i in top_vals:\n",
    "        word = vocab[i]\n",
    "        if word not in unique:\n",
    "            ind.append(i)\n",
    "            unique.add(vocab[i])\n",
    "            if len(unique) == k:\n",
    "                break\n",
    "    return ind\n",
    "\n",
    "\n",
    "\n",
    "def rank_freq(top_k_words, train_w_to_f_mult):\n",
    "    top_10_words = []\n",
    "    for words in top_k_words:\n",
    "        words = np.array(words)\n",
    "        count = np.array([len(train_w_to_f_mult[word]) for word in words ])\n",
    "        topk_vals = count.argsort()[-10:][::-1].astype(int)\n",
    "        top_10_words.append(words[topk_vals])\n",
    "    return top_10_words\n",
    "\n",
    "def rank_td_idf(top_k_words, tf_idf):\n",
    "    top_10_words = []\n",
    "    for words in top_k_words:\n",
    "        words = np.array(words)\n",
    "        count = np.array([tf_idf[word] for word in words ])\n",
    "        #topk_vals = count.argsort()[-10:][::-1].astype(int)\n",
    "        topk_vals = count.argsort()[-10:][::-1].astype(int)\n",
    "        top_10_words.append(words[topk_vals])\n",
    "    return top_10_words\n",
    "\n",
    "\n",
    "def rank_centrality(top_k_words, top_k, word_in_file):\n",
    "    for i, cluster in enumerate(top_k):\n",
    "        cluster = np.array(cluster)\n",
    "\n",
    "        subgraph = calc_coo_matrix(top_k_words[i], word_in_file)\n",
    "        G = nx.from_numpy_matrix(subgraph)\n",
    "        sc = nx.subgraph_centrality(G)\n",
    "\n",
    "        ind = np.argsort([sc[node] for node in sorted(sc)])[-10:][::-1].astype(int)\n",
    "\n",
    "\n",
    "        top_k_words[i] = np.array(top_k_words[i])[ind]\n",
    "    return top_k_words\n",
    "\n",
    "\n",
    "def calc_coo_matrix(word_intersect, word_in_file):\n",
    "    coo = np.zeros((len(word_intersect), len(word_intersect)))\n",
    "    for i in range(len(word_intersect)):\n",
    "        for j in range(i, len(word_intersect)):\n",
    "            coo[i, j] = count_wpair(word_intersect[i], word_intersect[j], word_in_file)\n",
    "            coo[j, i] = coo[i, j]\n",
    "    return coo\n",
    "\n",
    "def count_wpair(word1, word2, word_in_file):\n",
    "    combined_count = 0\n",
    "    if word1 != word2:\n",
    "        combined_count = len(set(word_in_file[word1]) & set(word_in_file[word2]))\n",
    "    return combined_count\n",
    "\n",
    "\n",
    "def find_words_for_cluster(m_clusters,  clusters):\n",
    "    indices = []\n",
    "    for i in range(0, clusters):\n",
    "        if i == -1:\n",
    "            continue\n",
    "        data_idx_within_i_cluster = [ idx for idx, clu_num in enumerate(m_clusters) if clu_num == i ]\n",
    "        indices.append(data_idx_within_i_cluster)\n",
    "    return indices\n",
    "\n",
    "def visualize(intersection):\n",
    "    intersection_red = TSNE_dim_reduction(intersection, 2)\n",
    "    for i in range(0,len(n_p)):\n",
    "        labels = np.where(labels==i, n_p[i], labels)\n",
    "    plt.scatter(intersection_red[:, 0], intersection_red[:, 1], c=labels, vmin=-0.5, vmax=0.5,  s=5, cmap='RdBu')\n",
    "\n",
    "    centers = np.empty(shape=(gmm.n_components, intersection_red.shape[1]))\n",
    "    for i in range(gmm.n_components):\n",
    "        density = scipy.stats.multivariate_normal(cov=gmm.covariances_[i], mean=gmm.means_[i]).logpdf(intersection)\n",
    "        centers[i, :] = intersection_red[np.argmax(density)]\n",
    "\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c=\"black\", s=35, alpha=0.7)\n",
    "    plt.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7325e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import reuters\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "\n",
    "def create_global_vocab(vocab_files):\n",
    "    vocab_list = set(line.split()[0] for line in open(vocab_files[0]))\n",
    "    for vocab in vocab_files:\n",
    "        vocab_list = vocab_list & set(line.split()[0] for line in open(vocab))\n",
    "    return vocab_list\n",
    "\n",
    "\n",
    "def combine_split_children(type):\n",
    "    files = []\n",
    "    index = 0\n",
    "    with open(os.path.join(DATADIR, 'CBTest/data/cbt_train.txt'), encoding='utf-8') as fp:\n",
    "        data = fp.readlines()\n",
    "    with open(os.path.join(DATADIR, 'CBTest/data/cbt_valid.txt'), encoding='utf-8') as fp:\n",
    "        data2 = fp.readlines()\n",
    "    with open(os.path.join(DATADIR, 'CBTest/data/cbt_test.txt'), encoding='utf-8') as fp:\n",
    "        data3 = fp.readlines()\n",
    "    data += \"\\n\"\n",
    "    data += data2\n",
    "    data += \"\\n\"\n",
    "    data += data3\n",
    "\n",
    "    for line in data:\n",
    "        words = line.strip()\n",
    "        if \"BOOK_TITLE\" in words:\n",
    "            continue\n",
    "        elif \"CHAPTER\" in words:\n",
    "            words = words.split()[2:]\n",
    "        else:\n",
    "            words = words.split()\n",
    "\n",
    "        if \"-RRB-\" in words:\n",
    "            words.remove(\"-RRB-\")\n",
    "        if \"-LRB-\" in words:\n",
    "            words.remove(\"-LRB-\")\n",
    "\n",
    "        sentence = (\" \".join(words) + \"\\n\")\n",
    "        if \"-RCB-\" in words:\n",
    "            sentence = sentence[0:sentence.find(\"-\")] + sentence[sentence.rfind(\"-\") + 1:]\n",
    "\n",
    "        if index % 20 == 0:\n",
    "            files.append(sentence)\n",
    "        else:\n",
    "            files[int(index / 20)] += sentence\n",
    "\n",
    "        index += 1\n",
    "    files = np.array(files)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    indices = list(kf.split(files))[0]\n",
    "\n",
    "    train_valid = files[indices[0]]\n",
    "    test = files[indices[1]]\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "    indices = list(kf.split(train_valid))[0]\n",
    "\n",
    "    train = train_valid[indices[0]]\n",
    "    valid = train_valid[indices[1]]\n",
    "    if type == \"train\":\n",
    "        return train\n",
    "    elif type == \"valid\":\n",
    "        return valid\n",
    "    else:\n",
    "        return test\n",
    "\n",
    "\n",
    "def create_files_20news(type):\n",
    "    if type == \"valid\":\n",
    "        type = \"test\"\n",
    "    data = fetch_20newsgroups(data_home='./data/', subset=type, remove=('headers', 'footers', 'quotes'))\n",
    "    files = data['data'];\n",
    "    return files\n",
    "\n",
    "\n",
    "def create_files_reuters(type):\n",
    "    t = type\n",
    "    if type == \"valid\":\n",
    "        t = \"test\"\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    id = [d for d in documents if d.startswith(t)]\n",
    "    files = np.array([reuters.raw(doc_id) for doc_id in id])\n",
    "\n",
    "    # if type != \"test\":\n",
    "    #     kf = KFold(n_splits=5, shuffle=True, random_state = 0)\n",
    "    #     indices = list(kf.split(files))[0]\n",
    "    #     train = files[indices[0]]\n",
    "    #     valid = files[indices[1]]\n",
    "\n",
    "    #     if type == \"train\":\n",
    "    #         return train\n",
    "    #     elif type == \"valid\":\n",
    "    #         return valid\n",
    "    return files\n",
    "\n",
    "\n",
    "def create_files_children(type):\n",
    "    files = combine_split_children(type)\n",
    "    return files\n",
    "\n",
    "\n",
    "def create_vocab_preprocess(stopwords, data, vocab, preprocess, process_data=False):\n",
    "    word_to_file = {}\n",
    "    word_to_file_mult = {}\n",
    "    strip_punct = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    strip_digit = str.maketrans(\"\", \"\", string.digits)\n",
    "\n",
    "    process_files = []\n",
    "    for file_num in range(0, len(data)):\n",
    "        words = data[file_num].lower().translate(strip_punct).translate(strip_digit)\n",
    "        words = words.split()\n",
    "        # words = [w.strip() for w in words]\n",
    "        proc_file = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in stopwords or (word not in vocab and len(vocab)) or word == \"dlrs\" or word == \"revs\":\n",
    "                continue\n",
    "            if word in word_to_file:\n",
    "                word_to_file[word].add(file_num)\n",
    "                word_to_file_mult[word].append(file_num)\n",
    "            else:\n",
    "                word_to_file[word] = set()\n",
    "                word_to_file_mult[word] = []\n",
    "\n",
    "                word_to_file[word].add(file_num)\n",
    "                word_to_file_mult[word].append(file_num)\n",
    "\n",
    "        process_files.append(proc_file)\n",
    "\n",
    "    for word in list(word_to_file):\n",
    "        if len(word_to_file[word]) <= preprocess or len(word) <= 3:\n",
    "            word_to_file.pop(word, None)\n",
    "            word_to_file_mult.pop(word, None)\n",
    "\n",
    "    print(\"Files:\" + str(len(data)))\n",
    "    print(\"Vocab: \" + str(len(word_to_file)))\n",
    "\n",
    "    if process_data:\n",
    "        vocab = word_to_file.keys()\n",
    "        files = []\n",
    "        for proc_file in process_files:\n",
    "            fil = []\n",
    "            for w in proc_file:\n",
    "                if w in vocab:\n",
    "                    fil.append(w)\n",
    "            files.append(\" \".join(fil))\n",
    "\n",
    "        data = files\n",
    "\n",
    "    return word_to_file, word_to_file_mult, data\n",
    "\n",
    "\n",
    "def create_vocab_and_files(stopwords, dataset, preprocess, type, vocab):\n",
    "    data = None\n",
    "    if dataset == \"20NG\":\n",
    "        data = create_files_20news(type)\n",
    "    elif dataset == \"children\":\n",
    "        data = create_files_children(type)\n",
    "    elif dataset == \"reuters\":\n",
    "        data = create_files_reuters(type)\n",
    "\n",
    "    return create_vocab_preprocess(stopwords, data, vocab, preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d4f3eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "import fasttext.util\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pdb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def create_id_dict(id2name):\n",
    "    data = {}\n",
    "    for line in open(id2name):\n",
    "        mapping = line.split()\n",
    "        data[mapping[0]] = mapping[1]\n",
    "    return data\n",
    "\n",
    "def read_entity_file(file, id_to_word, vocab, entities, elmomix=None):\n",
    "    data = []\n",
    "    word_index = {}\n",
    "    index = 0\n",
    "    mapping = None\n",
    "    if id_to_word != None:\n",
    "        mapping = create_id_dict(id_to_word)\n",
    "\n",
    "    if elmomix is None:\n",
    "        for line in open(file):\n",
    "            embedding = line.split()\n",
    "            if id_to_word != None:\n",
    "                embedding[0] = mapping[embedding[0]][1:]\n",
    "            if embedding[0] in vocab:\n",
    "                word_index[embedding[0]] = index\n",
    "                index +=1\n",
    "                if entities == \"glove\":\n",
    "                    embedding = list(map(float, embedding[-300:]))\n",
    "                else:\n",
    "                    embedding = list(map(float, embedding[1:]))\n",
    "                data.append(embedding)\n",
    "    else:  # specify mixing coefficients for ELMo\n",
    "        assert file[-1] in \"012\" and file[-7:-1] == \".layer\"\n",
    "        with open(file[:-1] + \"0\") as f0, open(file[:-1] + \"0\") as f0, open(file[:-1] + \"1\") as f1, open(file[:-1] + \"2\") as f2:\n",
    "            for l0, l1, l2 in zip(f0, f1, f2):\n",
    "                e0 = l0.split()\n",
    "                e1 = l1.split()\n",
    "                e2 = l2.split()\n",
    "                assert e0[0] == e1[0] and e1[0] == e2[0]\n",
    "                assert len(e0) == len(e1) and len(e1) == len(e2)\n",
    "                if id_to_word != None:\n",
    "                    e0[0] = mapping[e0[0]][1:]\n",
    "                    e1[0] = mapping[e1[0]][1:]\n",
    "                    e2[0] = mapping[e2[0]][1:]\n",
    "                if e0[0] in vocab:\n",
    "                    word_index[e0[0]] = index\n",
    "                    index +=1\n",
    "                    embedding = [elmomix[0] * float(x0) + elmomix[1] * float(x1) + elmomix[2] * float(x2) for x0, x1, x2 in zip(e0[1:], e1[1:], e2[1:])]\n",
    "                    data.append(embedding)\n",
    "\n",
    "    print(\"KG: \" + str(len(data)))\n",
    "    return data, word_index\n",
    "\n",
    "def create_doc_to_word_emb(word_to_doc, file_num, word_list, dim):\n",
    "    word_to_doc_matrix = np.zeros((len(word_list), file_num))\n",
    "    for i, word in enumerate(word_list):\n",
    "        for doc in word_to_doc[word]:\n",
    "            word_to_doc_matrix[i][doc] += 1\n",
    "\n",
    "    trun_ftw = TruncatedSVD(n_components=dim).fit_transform(word_to_doc_matrix)\n",
    "    return trun_ftw\n",
    "\n",
    "def find_intersect(word_index, vocab, data, files, type, add_doc):\n",
    "    if add_doc == \"DUP\":\n",
    "        return find_intersect_mult(word_index, vocab, data, type)\n",
    "    elif add_doc == \"SVD\":\n",
    "        intersection, words_index_intersect = find_intersect_unique(word_index, vocab, data, type)\n",
    "        u = create_doc_to_word_emb(vocab, files, words_index_intersect, 1000)\n",
    "        u = preprocessing.scale(u)\n",
    "        #intersection = np.concatenate((intersection, u), axis=1)\n",
    "        return u, words_index_intersect\n",
    "    else:\n",
    "        return find_intersect_unique(word_index, vocab, data, type)\n",
    "\n",
    "def find_intersect_unique(word_index, vocab, data, type):\n",
    "    words = []\n",
    "    vocab_embeddings = []\n",
    "\n",
    "    intersection = set(word_index.keys()) & set(vocab.keys())\n",
    "    print(\"Intersection: \" + str(len(intersection)))\n",
    "\n",
    "    intersection = np.sort(np.array(list(intersection)))\n",
    "    for word in intersection:\n",
    "        if type == \"word2vec\":\n",
    "            vocab_embeddings.append(data[word])\n",
    "        else:\n",
    "            vocab_embeddings.append(data[word_index[word]])\n",
    "        words.append(word)\n",
    "\n",
    "    vocab_embeddings = np.array(vocab_embeddings)\n",
    "\n",
    "    return vocab_embeddings, words\n",
    "\n",
    "def find_intersect_mult(word_index, vocab, data, type):\n",
    "    words = []\n",
    "    vocab_embeddings = []\n",
    "\n",
    "    intersection = set(word_index.keys()) & set(vocab.keys())\n",
    "    print(\"Intersection: \" + str(len(intersection)))\n",
    "\n",
    "    intersection = np.sort(np.array(list(intersection)))\n",
    "    for word in intersection:\n",
    "        for i in range(len(vocab[word])):\n",
    "            if type == \"word2vec\":\n",
    "                vocab_embeddings.append(data[word])\n",
    "            else:\n",
    "                vocab_embeddings.append(data[word_index[word]])\n",
    "            words.append(word)\n",
    "    print(len(words))\n",
    "    vocab_embeddings = np.array(vocab_embeddings)\n",
    "    return vocab_embeddings, words\n",
    "\n",
    "def create_entities_ft(model, train_word_to_file, doc_info):\n",
    "    #print(\"getting fasttext embeddings..\")\n",
    "    vocab_embeddings = []\n",
    "    words = []\n",
    "    intersection = set(train_word_to_file.keys())\n",
    "    for word in intersection:\n",
    "        if doc_info == \"DUP\":\n",
    "            for i in train_word_to_file[word]:\n",
    "                vocab_embeddings.append(model.get_word_vector(word))\n",
    "                words.append(word)\n",
    "        else:\n",
    "            vocab_embeddings.append(model.get_word_vector(word))\n",
    "            words.append(word)\n",
    "    vocab_embeddings = np.array(vocab_embeddings)\n",
    "    #print(\"complete..\")\n",
    "    return vocab_embeddings, words\n",
    "\n",
    "\n",
    "\n",
    "def get_weights_tf(vocab_list, weights):\n",
    "    return np.array([len(weights[w]) for w in vocab_list])\n",
    "\n",
    "def get_rs_weights_tf(vocab_list, wghts):\n",
    "    weights  = get_weights_tf(vocab_list, wghts)\n",
    "    transformer = RobustScaler().fit(get_weights_tf(vocab_list, wghts).reshape(-1, 1))\n",
    "    weight  = transformer.transform(weights.reshape(-1, 1))\n",
    "    x  = MinMaxScaler().fit(weight)\n",
    "    weights = (x.transform(weight)).T.squeeze()\n",
    "    return weights\n",
    "\n",
    "def get_weights_tfidf(vocab_list, weights):\n",
    "    return [weights[w] for w in vocab_list]\n",
    "def get_weights_tfdf(vocab_list, word_file_count, files_num):\n",
    "    count = np.array(get_weights_tf(vocab_list, word_file_count))\n",
    "    tf = count/np.sum(count)\n",
    "\n",
    "    df = np.array([len(np.unique(word_file_count[w])) for w in vocab_list])\n",
    "    df = df/files_num\n",
    "\n",
    "    weights = tf * df\n",
    "    print(weights.shape)\n",
    "\n",
    "    tfdf = {}\n",
    "    for i, w in enumerate(vocab_list):\n",
    "        tfdf[w]=weights[i]\n",
    "\n",
    "    return weights, tfdf\n",
    "def get_tfidf_score(data, train_vocab):\n",
    "    tf_idf_score = {}\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    total_tf_idf = tfidf_vectorizer_vectors.toarray().sum(axis=0)\n",
    "\n",
    "    vocab = set(words) & set(train_vocab.keys())\n",
    "    for i, word in enumerate(words):\n",
    "        if word in vocab:\n",
    "            tf_idf_score[word] = total_tf_idf[i]\n",
    "\n",
    "    return tf_idf_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d63867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def average_npmi_topics(topic_words, ntopics, word_doc_counts, nfiles):\n",
    "\n",
    "    eps = 10**(-12)\n",
    "\n",
    "    all_topics = []\n",
    "    for k in range(ntopics):\n",
    "        word_pair_counts = 0\n",
    "        topic_score = []\n",
    "\n",
    "        ntopw = len(topic_words[k])\n",
    "\n",
    "        for i in range(ntopw-1):\n",
    "            for j in range(i+1, ntopw):\n",
    "                w1 = topic_words[k][i]\n",
    "                w2 = topic_words[k][j]\n",
    "\n",
    "                w1w2_dc = len(word_doc_counts.get(w1, set()) & word_doc_counts.get(w2, set()))\n",
    "                w1_dc = len(word_doc_counts.get(w1, set()))\n",
    "                w2_dc = len(word_doc_counts.get(w2, set()))\n",
    "\n",
    "                # what we had previously:\n",
    "                #pmi_w1w2 = np.log(((w1w2_dc * nfiles) + eps) / ((w1_dc * w2_dc) + eps))\n",
    "\n",
    "                # Correct eps:\n",
    "                pmi_w1w2 = np.log((w1w2_dc * nfiles) / ((w1_dc * w2_dc) + eps) + eps)\n",
    "                npmi_w1w2 = pmi_w1w2 / (- np.log( (w1w2_dc)/nfiles + eps))\n",
    "\n",
    "                # Sanity check Which is equivalent to this:\n",
    "                #if w1w2_dc ==0:\n",
    "                #    npmi_w1w2 = -1\n",
    "                #else:\n",
    "                    #pmi_w1w2 = np.log( (w1w2_dc * nfiles)/ (w1_dc*w2_dc))\n",
    "                    #npmi_w1w2 = pmi_w1w2 / (-np.log(w1w2_dc/nfiles))\n",
    "\n",
    "                #if npmi_w1w2>1 or npmi_w1w2<-1:\n",
    "                #    print(\"NPMI score not bounded for:\", w1, w2)\n",
    "                #    print(npmi_w1w2)\n",
    "                #    sys.exit(1)\n",
    "\n",
    "                topic_score.append(npmi_w1w2)\n",
    "\n",
    "        all_topics.append(np.mean(topic_score))\n",
    "\n",
    "    for k in range(ntopics):\n",
    "        print(np.around(all_topics[k],5), \" \".join(topic_words[k]))\n",
    "\n",
    "    avg_score = np.around(np.mean(all_topics), 5)\n",
    "    #print(f\"\\nAverage NPMI for {ntopics} topics: {avg_score}\")\n",
    "\n",
    "    return avg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "94defda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import sys\n",
    "import argparse\n",
    "import string\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pdb\n",
    "import math\n",
    "import random\n",
    "\n",
    "NSEEDS = 5\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    stopwords = set(line.strip() for line in open('stopwords_en.txt'))\n",
    "\n",
    "    vocab = create_global_vocab(args.vocab)\n",
    "\n",
    "    train_word_to_file, train_w_to_f_mult, files = create_vocab_and_files(stopwords, args.dataset, args.preprocess, \"train\", vocab)\n",
    "    files_num = len(files)\n",
    "    print(\"len vocab size:\", len(train_word_to_file.keys()))\n",
    "\n",
    "    intersection = None\n",
    "    words_index_intersect = None\n",
    "\n",
    "    tf_idf = get_tfidf_score(files, train_word_to_file)\n",
    "\n",
    "    if args.entities == \"word2vec\":\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        intersection, words_index_intersect  = find_intersect(model.vocab,  train_w_to_f_mult, model, files_num, args.entities, args.doc_info)\n",
    "    elif args.entities == \"fasttext\":\n",
    "\n",
    "        # for compatibility, but move everything to embeds later.\n",
    "        #if os.path.exists('models/wiki.en.bin'):\n",
    "        #    ftfn = 'models/wiki.en.bin'\n",
    "        #else:\n",
    "        #\n",
    "        #ftfn = 'embeds/fasttext/wiki.en.bin'\n",
    "\n",
    "        #ft = fasttext.load_model(ftfn)\n",
    "        intersection, words_index_intersect = create_entities_ft(ft, train_w_to_f_mult, args.doc_info)\n",
    "        print(intersection.shape)\n",
    "    elif args.entities == \"KG\" or args.entities == \"glove\" :\n",
    "        elmomix = [float(a) for a in args.elmomix.split(\";\")] if args.elmomix != \"\" else None\n",
    "        data, word_index = read_entity_file(args.entities_file, args.id2name, train_word_to_file, args.entities, elmomix=elmomix)\n",
    "        intersection, words_index_intersect = find_intersect(word_index, train_w_to_f_mult, data, files_num, args.entities, args.doc_info)\n",
    "\n",
    "    if args.use_dims:\n",
    "        intersection = PCA_dim_reduction(intersection, args.use_dims)\n",
    "\n",
    "    #weights , tfdf = get_weights_tfdf(words_index_intersect, train_w_to_f_mult, files_num)\n",
    "    weights = None\n",
    "    tfdf = None\n",
    "\n",
    "\n",
    "    if args.doc_info == \"WGT\":\n",
    "        weights = get_weights_tf(words_index_intersect, train_w_to_f_mult)\n",
    "\n",
    "    if args.doc_info == \"robust\":\n",
    "        weights = get_rs_weights_tf(words_index_intersect, train_w_to_f_mult)\n",
    "\n",
    "    if args.doc_info == \"tfdf\":\n",
    "        weights , tfdf = get_weights_tfdf(words_index_intersect, train_w_to_f_mult, files_num)\n",
    "\n",
    "    if weights is not None and args.scale == \"sigmoid\":\n",
    "        print(\"scaling.. sigmoid\")\n",
    "        weights = 1 / (1 + np.exp(weights))\n",
    "\n",
    "\n",
    "    elif weights is not None and args.scale == \"log\":\n",
    "        print(\"scaling.. log\")\n",
    "        weights = np.log(weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dev_word_to_file, dev_word_to_file_mult, dev_files = create_vocab_and_files(stopwords, args.dataset,args.preprocess, \"valid\", vocab)\n",
    "    dev_files_num = len(dev_files)\n",
    "\n",
    "\n",
    "    test_word_to_file, test_word_to_file_mult, test_files = create_vocab_and_files(stopwords, args.dataset,args.preprocess, \"test\", vocab)\n",
    "    test_files_num = len(test_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    topics_npmi = []\n",
    "\n",
    "    for topics in args.num_topics:\n",
    "        npmis = []\n",
    "\n",
    "        print(\"Number of Clusters:\" + str(topics))\n",
    "        rand = 0\n",
    "        global NSEEDS\n",
    "        while rand < NSEEDS:\n",
    "\n",
    "            try:\n",
    "                top_k_words, top_k = cluster(args.clustering_algo, intersection, words_index_intersect, topics, args.rerank, weights, args.topics_file, rand)\n",
    "            except:\n",
    "                print(\"Warning: failed, try diff random seed.\")\n",
    "                new_rand = random.randint(5,1000)\n",
    "                top_k_words, top_k = cluster(args.clustering_algo, intersection, \\\n",
    "                        words_index_intersect, topics, args.rerank, weights, args.topics_file, new_rand)\n",
    "\n",
    "\n",
    "\n",
    "            top_k_words = rerank(args.rerank, top_k_words, top_k, train_w_to_f_mult, train_word_to_file, tf_idf, tfdf)\n",
    "            val = average_npmi_topics(top_k_words, len(top_k_words), dev_word_to_file, dev_files_num)\n",
    "\n",
    "            if np.isnan(val):\n",
    "                NSEEDS +=1\n",
    "                rand += 1\n",
    "                continue\n",
    "\n",
    "            npmi_score = np.around(val, 5)\n",
    "            print(\"NPMI:\" + str(npmi_score))\n",
    "            npmis.append(npmi_score)\n",
    "\n",
    "            rand += 1\n",
    "\n",
    "        topics_npmi.append(np.mean(npmis))\n",
    "        print(\"NPMI Mean:\" + str(np.around(topics_npmi[-1], 5)))\n",
    "        print(\"NPMI Var:\" + str(np.around(np.var(npmis), 5)))\n",
    "\n",
    "    best_topic = args.num_topics[np.argmax(topics_npmi)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cluster(clustering_algo, intersection, words_index_intersect, num_topics, rerank, weights, topics_file, rand):\n",
    "    if clustering_algo == \"KMeans\":\n",
    "        labels, top_k  = KMeans_model(intersection, words_index_intersect, num_topics, rerank, rand, weights)\n",
    "    elif clustering_algo == \"SPKMeans\":\n",
    "        labels, top_k  = SphericalKMeans_model(intersection, words_index_intersect, num_topics, rerank, rand, weights)\n",
    "    elif clustering_algo == \"GMM\":\n",
    "        labels, top_k = GMM_model(intersection, words_index_intersect, num_topics, rerank, rand)\n",
    "    elif clustering_algo == \"KMedoids\":\n",
    "        labels, top_k  = KMedoids_model(intersection,  words_index_intersect,  num_topics, rand)\n",
    "    elif clustering_algo == \"VMFM\":\n",
    "        labels, top_k = VonMisesFisherMixture_Model(intersection, words_index_intersect, num_topics, rerank, rand)\n",
    "\n",
    "    #Affinity matrix based\n",
    "    elif clustering_algo == \"DBSCAN\":\n",
    "        k=6\n",
    "        labels, top_k  = DBSCAN_model(intersection,k)\n",
    "    elif clustering_algo == \"Agglo\":\n",
    "        labels, top_k  = Agglo_model(intersecticlustering_algoon, num_topics, rand)\n",
    "    elif clustering_algo == \"Spectral\":\n",
    "        labels, top_k  = SpectralClustering_Model(intersection,num_topics, rand,  weights)\n",
    "\n",
    "    if clustering_algo == 'from_file':\n",
    "        with open('bert_topics.txt', 'r') as f:\n",
    "            top_k_words = f.readlines()\n",
    "        top_k_words = [tw.strip().replace(',', '').split() for tw in top_k_words]\n",
    "\n",
    "    elif clustering_algo == 'LDA':\n",
    "        with open(topics_file, 'r') as f:\n",
    "            top_k_words = f.readlines()\n",
    "        top_k_words = [tw.strip().replace(',', '').split() for tw in top_k_words]\n",
    "        for i, top_k in enumerate(top_k_words):\n",
    "            top_k_words[i] = top_k_words[i][2:12]\n",
    "    else:\n",
    "        bins, top_k_words = sort(labels, top_k,  words_index_intersect)\n",
    "    return top_k_words, np.array(top_k)\n",
    "\n",
    "\n",
    "def rerank(rerank, top_k_words, top_k, train_w_to_f_mult, train_w_to_f, tf_idf, tfdf):\n",
    "    if rerank==\"tf\":\n",
    "        top_k_words =  rank_freq(top_k_words, train_w_to_f_mult)\n",
    "        #top_k_words =  rank_freq(top_k_words, train_w_to_f)\n",
    "    elif rerank==\"tfidf\":\n",
    "        top_k_words = rank_td_idf(top_k_words, tf_idf)\n",
    "\n",
    "    elif rerank==\"tfdf\":\n",
    "        top_k_words = rank_td_idf(top_k_words, tfdf)\n",
    "\n",
    "    elif rerank==\"graph\":\n",
    "        #doc_matrix = npmi.calc_coo_matrix(words_index_intersect, train_word_to_file)\n",
    "        top_k_words = rank_centrality(top_k_words, top_k, train_w_to_f)\n",
    "    return top_k_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sort(labels, indices, word_index):\n",
    "    bins = {}\n",
    "    index = 0\n",
    "    top_k_bins = []\n",
    "    for label in labels:\n",
    "        if label not in bins:\n",
    "            bins[label] = [word_index[index]]\n",
    "        else:\n",
    "            bins[label].append(word_index[index])\n",
    "        index += 1;\n",
    "    for i in range(0, len(indices)):\n",
    "        ind = indices[i]\n",
    "        top_k = []\n",
    "        for word_ind in ind:\n",
    "            top_k.append(word_index[word_ind])\n",
    "        top_k_bins.append(top_k)\n",
    "    return bins, top_k_bins\n",
    "\n",
    "def print_bins(bins, name, type):\n",
    "    f = open(name + \"_\" + type + \"_corpus_bins.txt\",\"w+\")\n",
    "    for i in range(0, 20):\n",
    "        f.write(\"Bin \" + str(i) + \":\\n\")\n",
    "        for word in bins[i]:\n",
    "            f.write(word + \", \")\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "def print_top_k(top_k_bins, name, type):\n",
    "    f = open(name + \"_\" + type + \"_corpus_top_k.txt\",\"w+\")\n",
    "    for i in range(0, 20):\n",
    "        f.write(\"Bin \" + str(i) + \":\\n\")\n",
    "        top_k = top_k_bins[i]\n",
    "        for word in top_k:\n",
    "            f.write(word + \", \")\n",
    "        f.write(\"\\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "def parse_args(nb_dict = None) :\n",
    "    parser = argparse.ArgumentParser(__doc__)\n",
    "    parser.add_argument(\"--entities\", type=str, choices=[\"word2vec\", \"fasttext\", \"glove\", \"KG\"])\n",
    "    parser.add_argument( \"--entities_file\", type=str, help=\"entity file\")\n",
    "    parser.add_argument( \"--elmomix\", type=str, default=\"\", help=\"elmomix coefficients, separated by ';', should sum to 1\")\n",
    "\n",
    "    parser.add_argument(\"--clustering_algo\", type=str, required=True, choices=[\"KMeans\", \"SPKMeans\", \"GMM\", \"KMedoids\",\"Agglo\",\"DBSCAN\",\"Spectral\",\"VMFM\",\n",
    "        'from_file', 'LDA'])\n",
    "\n",
    "    parser.add_argument( \"--topics_file\", type=str, help=\"topics file\")\n",
    "\n",
    "    parser.add_argument('--use_dims', type=int)\n",
    "    parser.add_argument('--num_topics',  nargs='+', type=int, default=[20])\n",
    "    parser.add_argument(\"--doc_info\", type=str, choices=[\"SVD\", \"DUP\", \"WGT\", \"robust\", \\\n",
    "    \"logtfdf\"])\n",
    "    parser.add_argument(\"--rerank\", type=str, choices=[\"tf\", \"tfidf\", \"tfdf\", \"graph\"]) \\\n",
    "\n",
    "    parser.add_argument('--id2name', type=Path, help=\"id2name file\")\n",
    "\n",
    "    parser.add_argument(\"--dataset\", type=str, default =\"20NG\", choices=[\"20NG\", \"children\", \"reuters\"])\n",
    "\n",
    "    parser.add_argument(\"--preprocess\", type=int, default=5)\n",
    "    \n",
    "    parser.add_argument(\"--vocab\", required=True,  type=str, nargs='+', default=[])\n",
    "    parser.add_argument(\"--scale\", type=str, required=False)\n",
    "    \n",
    "    args_dict = dict()\n",
    "    if nb_dict is not None:\n",
    "        args_dict = nb_dict\n",
    "    else:\n",
    "        args_dict = {\n",
    "        \"entities\" : \"fasttext\",\n",
    "        \"clustering_algo\": \"KMeans\",\n",
    "        \"use_dims\": 2,\n",
    "        \"dataset\": \"20NG\",\n",
    "        \"vocab\": \"dict/english.txt\"\n",
    "    }\n",
    "    args_list = []\n",
    "    for k, v in args_dict.items():\n",
    "        args_list.append(f\"--{k}\")\n",
    "        args_list.append(str(v))\n",
    "\n",
    "\n",
    "    args = parser.parse_args(args_list)\n",
    "    #print(args)\n",
    "    return args\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9b9bd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"entities\" : \"fasttext\",\n",
    "    \"clustering_algo\": \"GMM\",\n",
    "    \"use_dims\": 2,\n",
    "    \"dataset\": \"20NG\",\n",
    "    \"vocab\": \"dict/english.txt\",\n",
    "    \"rerank\": \"tf\",\n",
    "    \"doc_info\": \"WGT\"\n",
    "    }\n",
    "args = parse_args(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aec07d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:11314\n",
      "Vocab: 11176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abijuru/.conda/envs/cemtom/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#args = parse_args()\n",
    "stopwords = set(line.strip() for line in open('stopwords_en.txt'))\n",
    "vocab = create_global_vocab(args.vocab)\n",
    "train_word_to_file, train_w_to_f_mult, files = create_vocab_and_files(stopwords, args.dataset, args.preprocess, \"train\", vocab)\n",
    "files_num = len(files)\n",
    "tf_idf = get_tfidf_score(files, train_word_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8514a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6a2899",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodel\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     ft \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "ft = None\n",
    "if model is not None:\n",
    "    ft = model.model\n",
    "else:\n",
    "    ft = fasttext.load_model(\"embeds/fasttext/wiki.en.bin\")\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e10857ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model(\"embeds/fasttext/wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9b1b024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection, words_index_intersect = create_entities_ft(ft, train_w_to_f_mult, args.doc_info)\n",
    "intersection = PCA_dim_reduction(intersection, args.use_dims)\n",
    "weights, tfdf = None, None\n",
    "if args.doc_info == \"WGT\":\n",
    "    weights = get_weights_tf(words_index_intersect, train_w_to_f_mult)\n",
    "\n",
    "if args.doc_info == \"robust\":\n",
    "    weights = get_rs_weights_tf(words_index_intersect, train_w_to_f_mult)\n",
    "\n",
    "if args.doc_info == \"tfdf\":\n",
    "    weights , tfdf = get_weights_tfdf(words_index_intersect, train_w_to_f_mult, files_num)\n",
    "\n",
    "if weights is not None and args.scale == \"sigmoid\":\n",
    "    print(\"scaling.. sigmoid\")\n",
    "    weights = 1 / (1 + np.exp(weights))\n",
    "\n",
    "\n",
    "elif weights is not None and args.scale == \"log\":\n",
    "    print(\"scaling.. log\")\n",
    "    weights = np.log(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d78f860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:11314\n",
      "Vocab: 11176\n",
      "Files:7532\n",
      "Vocab: 8622\n"
     ]
    }
   ],
   "source": [
    "dev_word_to_file, dev_word_to_file_mult, dev_files = create_vocab_and_files(stopwords, args.dataset,args.preprocess, \"train\", vocab)\n",
    "dev_files_num = len(dev_files)\n",
    "test_word_to_file, test_word_to_file_mult, test_files = create_vocab_and_files(stopwords, args.dataset,args.preprocess, \"test\", vocab)\n",
    "test_files_num = len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a643962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((20, 100), (20, 100))\n",
      "['light', 'feature', 'attached', 'stocks', 'booted', 'nuts', 'filling', 'exec', 'chewing', 'market', 'supplied', 'operations', 'photographic', 'flowing', 'commercial', 'cutting', 'rounded', 'freeze', 'squeezed', 'industrial', 'grips', 'airports', 'accessed', 'deployed', 'fingers', 'dice', 'terminator', 'tapping', 'sink', 'seed', 'clouds', 'checkers', 'locations', 'martian', 'foil', 'dump', 'drop', 'maneuver', 'tiny', 'vanilla', 'chains', 'approx', 'consists', 'firepower', 'handler', 'distributors', 'spark', 'seeds', 'halo', 'explosions', 'pile', 'astronauts', 'developer', 'video', 'astronautics', 'fahrenheit', 'tapes', 'squeeze', 'deposited', 'grain', 'engineered', 'jumps', 'trees', 'intercept', 'freezes', 'leak', 'consortium', 'prints', 'extra', 'small', 'explode', 'smart', 'companies', 'finger', 'washing', 'rebooted', 'foreground', 'swap', 'panel', 'notebook', 'kidney', 'sales', 'specialty', 'shallow', 'hooks', 'logo', 'worm', 'dribble', 'channel', 'drain', 'polished', 'tack', 'marker', 'fast', 'pull', 'intercepts', 'bone', 'transport', 'markets', 'recreational']\n",
      "[[ 3278 10318    87 ...  1810  8282  2773]\n",
      " [ 8516  5843  4800 ...  5932  6559   859]\n",
      " [ 4754   140  3591 ...  3722  8423  7821]\n",
      " ...\n",
      " [ 4190  4182  2972 ...  7352 10651  7744]\n",
      " [ 4430  4089  7808 ...  5458  8079  6770]\n",
      " [ 5599   305  7925 ...  7315  5492  1708]]\n",
      "0.20616 small video fast light market commercial extra companies feature sales\n",
      "0.2231 note makes pretty text whats talk purpose details condition reference\n",
      "0.3116 years april days return office received conference worked turned march\n",
      "0.12489 windows graphics hardware device modem surface load wiring spacecraft packages\n",
      "0.17967 accept belief scripture admit excuse discuss abuse deny terrorism arguing\n",
      "0.29512 encryption interface applications unix byte variable colormap desktop bitmap ascii\n",
      "0.22662 send works period position stop books present medical population hold\n",
      "0.06133 machine tape satellite vehicle clock shell heat installation plastic terminal\n",
      "0.31885 home street shot chicago boston angeles toronto club jose francisco\n",
      "-0.05736 speak criminal nazi fear afraid arabs nazis convince divine claiming\n",
      "0.19795 number power hard current include body complete weapons firearms natural\n",
      "0.24309 question reason questions evidence matter sense comments knowledge obvious answers\n",
      "0.16143 person thought interested anonymous supposed learn mention rule suspect conclusion\n",
      "0.23753 problem difference process standards risk properly determine documentation resolution tend\n",
      "0.21821 standard type monitor normal manual option programming costs produce handle\n",
      "-0.08273 games guns teams port built shots track super hitting jumpers\n",
      "0.28619 application tools functions method types external default tool cryptography reduce\n",
      "0.18591 crime gods leave secret party spirit civil claimed hurt officials\n",
      "0.17282 michael matthew college sunday richard signed senior islanders adam broke\n",
      "0.17343 service board sell library mine pass magazine training cross selling\n",
      "octis coherence: -0.3397401683031654\n",
      "NPMI:0.18419\n",
      "((20, 100), (20, 100))\n",
      "['catalogue', 'sensation', 'picture', 'suit', 'holders', 'sight', 'pour', 'divisions', 'insurance', 'compilation', 'lightly', 'arrangements', 'investors', 'employees', 'dates', 'editions', 'original', 'breed', 'paints', 'jobs', 'tossing', 'laying', 'start', 'missions', 'mouths', 'temporary', 'transplant', 'permanent', 'breaks', 'shaking', 'programme', 'terminated', 'european', 'profitable', 'casting', 'style', 'oranges', 'turn', 'acquiring', 'substitute', 'bleeding', 'exclusive', 'drawing', 'exchange', 'shifted', 'stamp', 'collections', 'absorbed', 'loose', 'occasionally', 'huge', 'sides', 'viewers', 'stopping', 'guided', 'digging', 'unbreakable', 'sleep', 'closes', 'squashed', 'offers', 'farming', 'fashion', 'primarily', 'buying', 'worlds', 'capture', 'sucked', 'overlap', 'grind', 'guide', 'bets', 'delayed', 'seller', 'disabled', 'stays', 'exciting', 'spells', 'date', 'alternate', 'special', 'fancy', 'draw', 'scattered', 'flags', 'long', 'shove', 'grin', 'tricks', 'locale', 'older', 'centres', 'highlights', 'instantly', 'bulletin', 'finalized', 'concentrated', 'creature', 'quickly', 'gradually']\n",
      "[[ 5346  8572  6365 ...  4317  2703  8638]\n",
      " [ 9297  6382  4030 ...  8227  8343 10530]\n",
      " [ 8468  4483  3204 ...  4057   984  7305]\n",
      " ...\n",
      " [ 1567  2546 10375 ...  2423   821  8891]\n",
      " [ 5574 11076  4727 ...  2184 11104  1910]\n",
      " [ 5877  6017  7974 ...   141  4457  4527]]\n",
      "0.18203 long start original turn special date jobs insurance picture offers\n",
      "0.16007 true person thought view learn rule reality amendment suspect status\n",
      "0.06095 package speed controller keyboard equipment radar manufacturers solar frame electrical\n",
      "0.23286 years started office weeks land conference america worked middle fall\n",
      "0.28127 simple error basic solution analysis object determine easier newsgroups requirements\n",
      "-0.05998 truth scripture conflict remark honest constitutional terrorism secular feelings propaganda\n",
      "0.13932 edge feet store floor hole seconds backup lock tank facility\n",
      "0.22168 disk hardware modem floppy disks motherboard configuration module desktop orbital\n",
      "0.1996 doesnt problems wouldnt sort sources scientific terms lack objective assuming\n",
      "0.23947 church love court brought lives meeting tells fight senate professor\n",
      "0.23394 large phone fast lines sound network commercial average volume communications\n",
      "0.1078 team road manager bought vancouver texas flames montreal riding seattle\n",
      "0.24255 system standard type rate monitor manual distribution programming costs handle\n",
      "0.23314 questions evidence agree argument comments knowledge policy ideas respect aware\n",
      "0.34455 data software server applications unix processing fonts encrypted servers modes\n",
      "0.23725 public general deal posted books past israeli administration present defense\n",
      "0.18315 bible rights asked knew calling evil eternal telling nazi peoples\n",
      "0.16536 info similar copy form directly sounds disease agencies mailing step\n",
      "0.19048 game running center side company front radio room pick door\n",
      "0.14674 york played west town smith brian chris lewis bruce angels\n",
      "octis coherence: -0.316965885093883\n",
      "NPMI:0.18711\n",
      "((20, 100), (20, 100))\n",
      "['games', 'pink', 'teams', 'supermarket', 'trench', 'bend', 'shelled', 'seals', 'suzuki', 'artillery', 'apollo', 'conductor', 'sugar', 'gate', 'dell', 'door', 'concussion', 'belly', 'restaurants', 'liner', 'apartments', 'pants', 'hammer', 'jeans', 'carpet', 'bridges', 'huts', 'housed', 'chocolate', 'bathroom', 'garaged', 'talon', 'spur', 'swinging', 'divisional', 'champaign', 'pilots', 'shoes', 'engineer', 'washed', 'orchid', 'chop', 'bomb', 'finish', 'crash', 'taped', 'salmon', 'hitters', 'designer', 'lakes', 'company', 'crossing', 'metro', 'buzz', 'slick', 'ship', 'entrance', 'wall', 'rain', 'bagged', 'grass', 'pistols', 'disney', 'highway', 'rabbit', 'studio', 'rubble', 'turf', 'collector', 'hitting', 'picks', 'crashes', 'scoring', 'bake', 'hurricane', 'antique', 'chest', 'tall', 'mines', 'roughing', 'makeshift', 'located', 'hotels', 'railway', 'shots', 'fleet', 'room', 'shield', 'rocks', 'heel', 'mint', 'sniper', 'front', 'punch', 'outdoors', 'guitar', 'pilot', 'pistol', 'bomber', 'builder']\n",
      "[[ 9755  8598  3359 ...  9717  5463  8771]\n",
      " [ 8302  6382  9297 ...  2683  3158  4084]\n",
      " [ 4151  4677  8603 ...  9333  4954  9776]\n",
      " ...\n",
      " [ 6147  5153  2081 ... 10119  4245  3123]\n",
      " [ 4834 11019 11032 ...  1089  8494  1764]\n",
      " [ 8326  6346  9476 ...  8473  5342  1741]]\n",
      "0.15458 games company front teams room door shots wall scoring located\n",
      "0.13004 true person thought interested learn rule reality amendment suspect status\n",
      "0.29796 disk hardware modem floppy disks motherboard configuration components module builtin\n",
      "0.26957 system files standard type rate product manual distribution options fixed\n",
      "0.21149 question reason matter sense assume knowledge suggest obvious answers suggestions\n",
      "0.14254 clinton michael college sunday named friday signed england adam islanders\n",
      "0.23012 change research stuff common whats purpose individual generally page fairly\n",
      "-0.06933 rights evil criminal nazi fear arabs nazis hitler divine revelation\n",
      "0.23927 years world started office early weeks turkey land conference month\n",
      "0.11691 package speed controller keyboard equipment orbit radar manufacturers cheaper solar\n",
      "0.16023 accept scripture admit excuse discuss conflict abuse deny terrorism arguing\n",
      "0.26604 level motif transfer developed table versions additional cheap slow prices\n",
      "0.29588 hockey shot street steve chicago boston angeles philadelphia club minnesota\n",
      "0.2991 simple error process specific analysis object determine defined newsgroups accurate\n",
      "-0.07633 keys drivers water apple installed circuit seconds plane tanks backup\n",
      "0.18492 love court judge meeting tells senate professor sick forced citizen\n",
      "0.20647 life book hope israel history attack congress community happy youd\n",
      "0.27986 data software encryption applications unix encrypted compatible modes toolkit mechanism\n",
      "0.16692 list found works making advance excellent changed events media perfect\n",
      "0.15494 mail total performance class forward moving piece heavy deep echo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octis coherence: -0.33038367043209754\n",
      "NPMI:0.18306\n",
      "((20, 100), (20, 100))\n",
      "['holiday', 'fell', 'riot', 'anton', 'college', 'legion', 'goodbye', 'ronald', 'cheer', 'vera', 'crowley', 'aged', 'gang', 'broke', 'jeffrey', 'signed', 'alexandria', 'dressed', 'herbert', 'luke', 'mccarthy', 'bombed', 'served', 'janet', 'legend', 'wallach', 'retirement', 'battle', 'legends', 'returning', 'arrives', 'miss', 'darius', 'devil', 'clinton', 'shortly', 'girl', 'stole', 'truelove', 'potter', 'resident', 'residence', 'tribune', 'queen', 'bury', 'calvin', 'adam', 'grew', 'birthday', 'commander', 'sunday', 'graves', 'singer', 'friday', 'islanders', 'michael', 'senior', 'warriors', 'roommate', 'village', 'buried', 'england', 'career', 'jude', 'bout', 'angel', 'hells', 'curt', 'anniversary', 'noah', 'palace', 'dawn', 'steven', 'nurse', 'levi', 'girls', 'parish', 'nickname', 'alan', 'webster', 'arriving', 'rally', 'dinner', 'joshua', 'trinity', 'arrival', 'wills', 'guests', 'named', 'papa', 'rushed', 'saint', 'isaac', 'guest', 'felix', 'juan', 'kenneth', 'woke', 'keeper', 'earned']\n",
      "[[ 6469  5456  1012 ...  5511  5562    29]\n",
      " [ 4098  9692  9177 ...  4745   948  8766]\n",
      " [  329  7705  7469 ...  7653  8678  1628]\n",
      " ...\n",
      " [ 3021  7702  7762 ...  1657  1106   413]\n",
      " [ 2029  4825  6462 ... 10813  3608  7095]\n",
      " [ 2227  2410  1903 ...  7907  5630  2256]]\n",
      "0.15229 clinton michael college sunday named career friday signed village senior\n",
      "0.2502 similar internet numbers directory easy images needed create quality easily\n",
      "0.2559 call general written future continue bring armenia calls trouble majority\n",
      "0.20793 type technology format produce font energy weight install bytes solid\n",
      "-0.03016 guns built shots flight track super tech square pool jumpers\n",
      "0.2048 anonymous conclusion personally social knowing governments persons speech replies parties\n",
      "0.20397 problem information problems sort correct cases apply suppose messages risk\n",
      "0.2194 hockey shot street steve chicago boston angeles philadelphia club riding\n",
      "0.19031 hand fine entry advance date entire shows modern receive property\n",
      "0.25815 software encryption server applications unix processing encrypted widgets byte modes\n",
      "0.20763 didnt case thing support kind word message important simply wont\n",
      "0.22695 news couple press country return private coming finally title hands\n",
      "0.25348 asked crime knew wrote turks civil leaders talked tradition worship\n",
      "0.22738 truth religious political genocide islam conflict believes violence secular feelings\n",
      "0.21727 application tools method bios external default tool easier objects reduce\n",
      "0.22474 half record arms magazine owners selling released heads annual central\n",
      "0.02759 chip machine satellite vehicle clock shell heat installation plastic terminal\n",
      "0.15617 disk hardware floppy disks motherboard wiring spacecraft configuration module plug\n",
      "0.21574 file large phone single sound light commercial volume safety extra\n",
      "0.22544 questions evidence agree argument comments policy ideas respect aware absolutely\n",
      "octis coherence: -0.35227374502172804\n",
      "NPMI:0.19476\n",
      "((20, 100), (20, 100))\n",
      "['spike', 'flyer', 'winds', 'built', 'rocks', 'swing', 'guns', 'vault', 'cracker', 'grass', 'pounds', 'ship', 'bullets', 'outdoors', 'shots', 'shotgun', 'beacon', 'giant', 'horns', 'climb', 'shops', 'swinging', 'neptune', 'canal', 'furniture', 'bridges', 'finishes', 'entryway', 'crashes', 'super', 'chocolate', 'liner', 'jumpers', 'roll', 'bagged', 'punch', 'pants', 'belly', 'hollow', 'carpet', 'hiking', 'nonstop', 'pistol', 'volcano', 'gate', 'centaur', 'dell', 'shoes', 'passengers', 'route', 'goaltenders', 'talon', 'marine', 'apollo', 'suzuki', 'sail', 'track', 'metro', 'breakers', 'garaged', 'iron', 'dirt', 'designer', 'jeans', 'hitters', 'ocean', 'bend', 'towers', 'averaged', 'neck', 'watts', 'pool', 'crashing', 'teams', 'hitting', 'chest', 'sand', 'elbow', 'rolling', 'doors', 'concussion', 'lantern', 'standings', 'rounds', 'wall', 'square', 'picks', 'rubble', 'palm', 'hurricane', 'artillery', 'shoulder', 'dortmund', 'tech', 'games', 'tall', 'cage', 'cream', 'jumbo', 'exiting']\n",
      "[[ 4459  8932  8317 ...  8646  6380  1188]\n",
      " [ 4059  4998  3075 ...  1639  4516  2571]\n",
      " [ 2360  1362  5102 ...  3100  8312  7631]\n",
      " ...\n",
      " [ 4825  4904  2029 ...  1017  7095  2242]\n",
      " [ 6155  4267  6745 ... 10558  3704  7589]\n",
      " [ 2286   796  3053 ... 11139  5688  4383]]\n",
      "0.05228 games guns teams built shots track wall super tech square\n",
      "0.18114 person thought interested care considered interest supposed learn mention forget\n",
      "0.08301 installed satellite wire vehicle clock shell heat installation plastic terminal\n",
      "0.24594 application tools method bios external default tool objects reduce depending\n",
      "0.08786 court turks party judge tells sick taught satan citizen jerusalem\n",
      "0.0412 truth scripture excuse conflict deny remark secular terrorism feelings propaganda\n",
      "0.2197 point list free hand making fine entry excellent entire secure\n",
      "0.21149 case means address kind simply wont interesting whats talk purpose\n",
      "0.19523 state post money turkish private months face taking passed luck\n",
      "0.13811 disk hardware floppy surface disks motherboard wiring spacecraft configuration module\n",
      "0.23497 question subject questions evidence comments knowledge ideas false respect absolutely\n",
      "0.26776 home hockey shot street steve chicago boston angeles philadelphia club\n",
      "0.17804 technology size format font energy weight install bytes solid noise\n",
      "0.16251 numbers directory easy images quality higher easily patients convert escrow\n",
      "0.25815 software encryption server applications unix processing encrypted widgets byte modes\n",
      "0.1351 rights calling evil criminal telling nazi fear peoples arabs writes\n",
      "0.2671 lost school clinton spent college director grant career friday signed\n",
      "0.20951 file large phone single sound light market commercial volume safety\n",
      "0.19601 problem information correct theory apply suppose risk documentation resolution tend\n",
      "0.18915 half record magazine owners cross selling session heads starts edition\n",
      "octis coherence: -0.3583399189736071\n",
      "NPMI:0.17771\n",
      "NPMI Mean:0.18537\n",
      "NPMI Var:3e-05\n"
     ]
    }
   ],
   "source": [
    "topics_npmi = []\n",
    "#tf_idf, tfdf = None, None\n",
    "octis_cohere = Coherence(texts=[doc.split() for doc in test_corpus])\n",
    "for topics in args.num_topics:\n",
    "    npmis = []\n",
    "    rand = 0\n",
    "    global NSEEDS\n",
    "    while rand < NSEEDS:\n",
    "        top_k_words, top_k = cluster(\n",
    "        args.clustering_algo, intersection, \n",
    "        words_index_intersect, topics, args.rerank, \n",
    "        weights, args.topics_file, rand)\n",
    "        print((np.array(top_k_words).shape, top_k.shape))\n",
    "        #print(top_k_words)\n",
    "        print(top_k_words[0])\n",
    "        print(top_k)\n",
    "        top_k_words = rerank(args.rerank, top_k_words, top_k, train_w_to_f_mult, train_word_to_file, tf_idf, tfdf)\n",
    "        val = average_npmi_topics(top_k_words, len(top_k_words), dev_word_to_file, dev_files_num)\n",
    "        octis_coh = octis_cohere.score({'topics': top_k_words})\n",
    "        print(f\"octis coherence: {octis_coh}\")\n",
    "        if np.isnan(val):\n",
    "            NSEEDS +=1\n",
    "            rand += 1\n",
    "            continue\n",
    "        npmi_score = np.around(val, 5)\n",
    "        print(\"NPMI:\" + str(npmi_score))\n",
    "        npmis.append(npmi_score)\n",
    "\n",
    "        rand += 1\n",
    "    topics_npmi.append(np.mean(npmis))\n",
    "    print(\"NPMI Mean:\" + str(np.around(topics_npmi[-1], 5)))\n",
    "    print(\"NPMI Var:\" + str(np.around(np.var(npmis), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0800fd82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "for topic in 5:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0cd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cemtom] *",
   "language": "python",
   "name": "conda-env-.conda-cemtom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
